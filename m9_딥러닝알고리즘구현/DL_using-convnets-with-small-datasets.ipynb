{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_using-convnets-with-small-datasets.ipynb","provenance":[],"collapsed_sections":["fklgjRQ2Rmfj"],"toc_visible":true,"mount_file_id":"1lJIEZ8Z-vFSiXZ0yO88X8p5gVy7SC_eM","authorship_tag":"ABX9TyNSxuofSQ3TqK5yE1C6cEpa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["### 버전 변경"],"metadata":{"id":"fklgjRQ2Rmfj"}},{"cell_type":"code","source":["!pip install keras==2.3.1\n","!pip install tensorflow==2.2.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJM5xZtVRUaN","executionInfo":{"status":"ok","timestamp":1650699881838,"user_tz":-540,"elapsed":85457,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}},"outputId":"0bbfeb3c-a86e-4c2b-e238-85d9bba1207b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting keras==2.3.1\n","  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\n","\u001b[K     |████████████████████████████████| 377 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.1.2)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.4.1)\n","Collecting keras-applications>=1.0.6\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.21.6)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (3.1.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.3.1) (1.15.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.3.1) (1.5.2)\n","Installing collected packages: keras-applications, keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.8.0\n","    Uninstalling keras-2.8.0:\n","      Successfully uninstalled keras-2.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","tensorflow 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.3.1 which is incompatible.\u001b[0m\n","Successfully installed keras-2.3.1 keras-applications-1.0.8\n","Collecting tensorflow==2.2.0\n","  Downloading tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2 MB)\n","\u001b[K     |████████████████████████████████| 516.2 MB 4.4 kB/s \n","\u001b[?25hCollecting gast==0.3.3\n","  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.14.0)\n","Collecting tensorflow-estimator<2.3.0,>=2.2.0\n","  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n","\u001b[K     |████████████████████████████████| 454 kB 29.5 MB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.2.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.0.0)\n","Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.6.3)\n","Collecting tensorboard<2.3.0,>=2.2.0\n","  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 34.9 MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.44.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.21.6)\n","Collecting h5py<2.11.0,>=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 46.0 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (3.17.3)\n","Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.4.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (0.37.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.2.0) (1.1.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (57.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.6)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.35.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2021.10.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.0)\n","Installing collected packages: tensorflow-estimator, tensorboard, h5py, gast, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.8.0\n","    Uninstalling tensorboard-2.8.0:\n","      Successfully uninstalled tensorboard-2.8.0\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.8.0\n","    Uninstalling tensorflow-2.8.0:\n","      Successfully uninstalled tensorflow-2.8.0\n","Successfully installed gast-0.3.3 h5py-2.10.0 tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kHsA0vaPSDBk","executionInfo":{"status":"ok","timestamp":1650700270633,"user_tz":-540,"elapsed":461,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}},"outputId":"fd8f61b1-add6-46b5-eb4a-62a1e6e60fad"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.2.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import keras\n","keras.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"9fnmRH5tS6e7","executionInfo":{"status":"ok","timestamp":1650700279749,"user_tz":-540,"elapsed":7,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}},"outputId":"b38bfc1d-d63e-4b02-c6cd-bb736cec5a63"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Using TensorFlow backend.\n"]},{"output_type":"execute_result","data":{"text/plain":["'2.3.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# 작은 데이터셋 문제에서 딥러닝의 타당성\n","\n","딥러닝은 데이터가 풍부할 때만 작동한다는 말을 이따금 듣는다. 부분적으로는 맞는 이야기이다. 딥러닝의 근본적인 특징은 훈련 데이터에서 특성 공학의 수작업 없이 흥미로운 특성을 찾을 수 있다. 이는 훈련 샘플이 많아야만 가능하다. 입력 샘플이 이미지와 같이 매우 고차원인 문제에서는 특히 그렇다.\n","\n","- 많은 샘플이 의미하는 것은 상대적이다.\n","\n","\n","- 복잡한 문제를 푸는 컨브넷을 수십 개의 샘플만을 사용해서 훈련하는 것은 불가능하다.\n","\n","- 하지만 모델이 작고 규제가 잘 되어 있으며 간단한 작업이라면 수백 개의 샘플로도 충분할 수 있다.\n","\n","- 컨브넷은 지역적이고 평행 이동으로 변하지 않는 특성을 학습하기 때문에 지각에 관한 문제에서 매우 효율적으로 데이터를 사용한다.\n","\n","\n","- 매우 작은 이미지 데이터셋에서 어떤 종류의 특성 공학을 사용하지 않고 컨브넷을 처음부터 훈련해도 납득할 만한 결과를 만들 수 있다.\n","\n","\n","- 딥러닝 모델은 태생적으로 매우 다목적이다.\n","\n","\n","- 대규모 데이터셋에서 훈련시킨 이미지 분류 모델이나 스피치-투-텍스트 모델을 조금만 변경해서 완전히 다른 문제에 재사용할 수 있다.\n","\n","\n","- 특히 컴퓨터 비전에서는 (보통 ImageNet 데이터셋에서 훈련된) 사전 훈련된 모델들이 다운로드받을 수 있도록 많이 공개되어 있어서 매우 적은 데이터에서 강력한 비전 모델을 만드는데 사용할 수 있다.\n"],"metadata":{"id":"t3JYkGXgk25B"}},{"cell_type":"markdown","source":["# 데이터 로드\n","### 소규모 데이터 셋에서 컨브넷 훈련하기 \n","\n","4000개의 강아지 고양이 사진으로 구성된 데이터셋을 train 2000, validation 1000, test 1000개로 분리하여 사용.\n","\n","- 과대 적합 여부 확인\n","- 데이터 증식을 통한 네트워크의 성능 개선\n","- 사전 훈련된 네트워크을 활용"],"metadata":{"id":"HMuW08KCnDsw"}},{"cell_type":"code","source":["import os, shutil\n","base_dir = './drive/MyDrive/cakd5_colab/m9_딥러닝알고리즘구현/cats_and_dogs_small'\n","\n","\n","# 훈련, 검증, 테스트 분할을 위한 디렉터리\n","train_dir = os.path.join(base_dir, 'train')\n","\n","validation_dir = os.path.join(base_dir, 'validation')\n","\n","test_dir = os.path.join(base_dir, 'test')\n","\n","\n","# 훈련용 고양이 사진 디렉터리\n","train_cats_dir = os.path.join(train_dir, 'cats')\n","\n","\n","# 훈련용 강아지 사진 디렉터리\n","train_dogs_dir = os.path.join(train_dir, 'dogs')\n","\n","\n","# 검증용 고양이 사진 디렉터리\n","validation_cats_dir = os.path.join(validation_dir, 'cats')\n","\n","\n","# 검증용 강아지 사진 디렉터리\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n","\n","\n","# 테스트용 고양이 사진 디렉터리\n","test_cats_dir = os.path.join(test_dir, 'cats')\n","\n","\n","# 테스트용 강아지 사진 디렉터리\n","test_dogs_dir = os.path.join(test_dir, 'dogs')\n"],"metadata":{"id":"p8orTcciTBlE","executionInfo":{"status":"ok","timestamp":1650700766428,"user_tz":-540,"elapsed":550,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print('훈련용 고양이 이미지 전체 개수:', len(os.listdir(train_cats_dir)))\n","print('훈련용 강아지 이미지 전체 개수:', len(os.listdir(train_dogs_dir)))\n","print('검증용 고양이 이미지 전체 개수:', len(os.listdir(validation_cats_dir)))\n","print('검증용 강아지 이미지 전체 개수:', len(os.listdir(validation_dogs_dir)))\n","print('테스트용 고양이 이미지 전체 개수:', len(os.listdir(test_cats_dir)))\n","print('테스트용 강아지 이미지 전체 개수:', len(os.listdir(test_dogs_dir)))"],"metadata":{"id":"Fy9J6WPRTBcK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650700839366,"user_tz":-540,"elapsed":477,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}},"outputId":"7768fcdf-1823-41eb-c361-592363e2c4ad"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련용 고양이 이미지 전체 개수: 1000\n","훈련용 강아지 이미지 전체 개수: 1000\n","검증용 고양이 이미지 전체 개수: 500\n","검증용 강아지 이미지 전체 개수: 500\n","테스트용 고양이 이미지 전체 개수: 500\n","테스트용 강아지 이미지 전체 개수: 500\n"]}]},{"cell_type":"markdown","source":["# 네트워크 구성"],"metadata":{"id":"yORudhCAq3EH"}},{"cell_type":"markdown","source":["Conv2D(relu 활성화 함수 사용)와 MaxPooling2D 층을 번갈아 쌓은 컨브넷을 만든다.\n","\n","\n","이전보다 이미지가 크고 복잡한 문제이기 때문에 네트워크를 좀 더 크게 만든다.\n","\n","\n","Conv2d + MaxPooling2D 단계를 하나 더 추가한다.\n","\n","\n","이렇게 하면 네트워크의 용량을 늘리고 Flatten 층의 크기가 너무 커지지 않도록 특성 맵의 크기를 줄일 수 있다.\n","\n","\n","150 × 150 크기(임의로 선택한 것)의 입력으로 시작해서 Flatten 층 이전에 7 × 7 크기의 특성 맵으로 줄어든다.\n","\n","\n","특성 맵의 깊이는 네트워크에서 점진적으로 증가하지만(32에서 128까지), 특성 맵의 크기는 감소한다(150 × 150 에서 7 × 7까지). 이는 거의 모든 컨브넷에서 볼 수 있는 전형적인 패턴이다.\n","\n","\n","이진 분류 문제이므로 네트워크는 하나의 유닛(크기가 1인 Dense 층)과 sigmoid 활성화 함수로 끝난다. 이 유닛은 한 클래스에 대한 확률을 인코딩할 것이다."],"metadata":{"id":"oRfqNr1wcQSv"}},{"cell_type":"code","source":["from keras import layers, models\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))\n","model.add(layers.MaxPooling2D((2,2)))\n","model.add(layers.Conv2D(64,(3,3), activation='relu'))\n","model.add(layers.MaxPooling2D((2,2)))\n","model.add(layers.Conv2D(128,(3,3), activation='relu'))\n","model.add(layers.MaxPooling2D((2,2)))\n","model.add(layers.Conv2D(128,(3,3), activation='relu'))\n","model.add(layers.MaxPooling2D((2,2)))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(1,activation='sigmoid'))"],"metadata":{"id":"Bn-uIeRMkvJl","executionInfo":{"status":"ok","timestamp":1650701476215,"user_tz":-540,"elapsed":1959,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"9R8eQJC0pJnx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650701480053,"user_tz":-540,"elapsed":812,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}},"outputId":"37e1467f-1132-4c4a-89e6-dce8b13a182b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 6272)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               3211776   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 513       \n","=================================================================\n","Total params: 3,453,121\n","Trainable params: 3,453,121\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from keras import optimizers\n","\n","model.compile(loss = 'binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])"],"metadata":{"id":"a2ARfDCFpJkY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["컴파일 단계에서 이전과 같이 RMSprop optimizer을 선택한다. \n","\n","네트워크의 마지막이 하나의 sigmoid 유닛이기 때문에 이진 크로스엔트로피(binary crossentropy)를 손실로 사용한다."],"metadata":{"id":"BEOYQ5WDeyJj"}},{"cell_type":"markdown","source":["# 데이터 전처리"],"metadata":{"id":"uyU-CYPAq6_J"}},{"cell_type":"markdown","source":["데이터는 네트워크에 주입되기 전에 부동 소수 타입의 텐서로 적절하게 전처리되어야 한다.\n","\n","지금은 데이터가 JPEG 파일로 되어 있으므로 네트워크에 주입하려면 대략 다음 과정을 따른다.\n","\n","1. 사진 파일을 읽습니다.\n","2. JPEG 콘텐츠를 RGB 픽셀 값으로 디코딩한다.\n","3. 그 다음 부동 소수 타입의 텐서로 변환한다.\n","4. 픽셀 값(0에서 255 사이)의 스케일을 [0, 1] 사이로 조정한다(신경망은 작은 입력 값을 선호한).\n","\n","\n","좀 복잡하게 보일 수 있지만 다행히 케라스는 이런 단계를 자동으로 처리하는 유틸리티를 가지고 있다.\n","\n","케라스는 keras.preprocessing.image에 이미지 처리를 위한 헬퍼 도구들을 가지고 있다.\n","\n","\n","특히 ImageDataGenerator 클래스는 디스크에 있는 이미지 파일을 전처리된 배치 텐서로 자동으로 바꾸어주는 파이썬 제너레이터를 만들어 준다."],"metadata":{"id":"sUKYp-LDfAl4"}},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","# 모든 이미지를 1/255로 스케일을 조정\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","        # 타깃 디렉터리\n","        train_dir,\n","        # 모든 이미지를 150 × 150 크기로 변경\n","        target_size=(150, 150),\n","        batch_size=20,\n","        # binary_crossentropy 손실을 사용하기 때문에 이진 레이블이 필요함\n","        class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(150, 150),\n","        batch_size=20,\n","        class_mode='binary')"],"metadata":{"id":"ZYgtcL0epJWo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650701850637,"user_tz":-540,"elapsed":497,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}},"outputId":"b961762a-6ed7-4035-f893-360ef6fe46d2"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"]}]},{"cell_type":"markdown","source":["이 제너레이터의 출력을 하나 확인해본다.\n","\n","이 출력은 150 × 150 RGB 이미지의 배치((20, 150, 150, 3) 크기)와 이진 레이블의 배치((20,) 크기)이다.\n","\n","각 배치에는 20개의 샘플(배치 크기)이 있다.\n","\n","제너레이터는 이 배치를 무한정 만들어 낸다.\n","\n","타깃 폴더에 있는 이미지를 끝없이 반복한다. 따라서 반복 루프안의 어디에선가 break 문을 사용해야 한다. "],"metadata":{"id":"A4rk_MQxf2y4"}},{"cell_type":"code","source":["for data_batch, labels_batch in train_generator:\n","    print('배치 데이터 크기:', data_batch.shape)\n","    print('배치 레이블 크기:', labels_batch.shape)\n","    break"],"metadata":{"id":"mRy4gcvVpJNZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650701867158,"user_tz":-540,"elapsed":12085,"user":{"displayName":"무말랭이","userId":"13893909045648027159"}},"outputId":"98c6ad38-acd2-4d60-c80d-88fbab23cddf"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["배치 데이터 크기: (20, 150, 150, 3)\n","배치 레이블 크기: (20,)\n"]}]},{"cell_type":"markdown","source":["- fit_generator 메서드는 fit 메서드와 동일하되 데이터 제너레이터를 사용할 수 있다.\n","\n","\n","- 이 메서드는 첫 번째 매개변수로 입력과 타깃의 배치를 끝없이 반환하는 파이썬 제너레이터를 기대한다.\n","\n","\n","- 데이터가 끝없이 생성되기 때문에 케라스 모델에 하나의 에포크를 정의하기 위해 제너레이터로부터 얼마나 많은 샘플을 뽑을 것인지 알려 주어야 한다.\n","\n","\n","- steps_per_epoch 매개변수에서 이를 설정한다.\n","\n","\n","- 제너레이터로부터 steps_per_epoch 개의 배치만큼 뽑은 다음, 즉 steps_per_epoch 횟수만큼 경사 하강법 단계를 실행한 다음에 훈련 프로세스는 다음 에포크로 넘어간다.\n","\n","\n","- 20개의 샘플이 하나의 배치이므로 2,000개의 샘플을 모두 처리할 때까지 100개의 배치를 뽑을 것이다.\n","\n","\n","- fit_generator를 사용할 때 fit 메서드와 마찬가지로 validation_data 매개변수를 전달할 수 있다.\n","\n","\n","- 이 매개변수에는 데이터 제너레이터도 가능하지만 넘파이 배열의 튜플도 가능하다.\n","\n","\n","- validation_data로 제너레이터를 전달하면 검증 데이터의 배치를 끝없이 반환한다.\n","\n","\n","- 따라서 검증 데이터 제너레이터에서 얼마나 많은 배치를 추출하여 평가할지 validation_steps 매개변수에 지정해야 한다."],"metadata":{"id":"SeVnsiUuqNf3"}},{"cell_type":"code","source":["history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=100,\n","    epochs=30,\n","    validation_data=validation_generator,\n","    validation_steps=50\n",")"],"metadata":{"id":"JGMQAuALqMD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('./drive/MyDrive/cakd5_colab/m9_딥러닝알고리즘구현/cats_and_dogs_small_1.h5')"],"metadata":{"id":"F2ntfQxcqMBC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["훈련이 끝나면 항상 모델을 저장하는것이 좋은 습관이다."],"metadata":{"id":"-jhONBICuoaX"}},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('./drive/MyDrive/cakd5_colab/m9_딥러닝알고리즘구현/cats_and_dogs_small_1.h5')"],"metadata":{"id":"6rTr137KqL-0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델의 손실과 정확도 시각화"],"metadata":{"id":"9evYpDcnuypn"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc'] \n","loss = history.history['loss'] \n","val_loss = history.history['val_loss'] \n","\n","epochs  = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"qV51xh9DqL8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 그래프는 과대적합의 특성을 보인다.\n","\n","\n","훈련 정확도가 시간이 지남에 따라 선형적으로 증가해서 거의 100%에 도달한다.\n","\n","\n","반면 검증 정확도는 70-72%에서 멈추었다.\n","\n","\n","검증 손실은 다섯 번의 에포크만에 최솟값에 다다른 이후에 더 이상 진전되지 않았다.\n","\n","\n","반면 훈련 손실은 거의 0에 도달할 때까지 선형적으로 계속 감소한다.\n","\n","\n","비교적 훈련 샘플의 수(2,000개)가 적기 때문에 과대적합이 가장 중요한 문제이다.\n","\n","\n","이전 예제에서 드롭아웃이나 가중치 감소(L2 규제)와 같은 과대적합을 감소시킬 수 있는 여러 가지 기법들을 배웠다.\n","\n","\n","지금 예제에서는 컴퓨터 비전에 특화되어 있어서 딥러닝으로 이미지를 다룰 때 매우 일반적으로 사용되는 새로운 방법인 데이터 증식을 시도해본다."],"metadata":{"id":"d-pIyUqrvyd_"}},{"cell_type":"markdown","source":["### 참조\n","Keras 및 Tensorflow를 사용한 이미지 데이터 증대방안\n","\n","https://ichi.pro/ko/keras-mich-tensorflowleul-sayonghan-imiji-deiteo-jeungdae-tamsaeg-184813206747204"],"metadata":{"id":"urXQQjWQqorX"}},{"cell_type":"markdown","source":["# 데이터 증식하여 모델 학습, 평가 및 시각화"],"metadata":{"id":"pHOBbEGCqrmX"}},{"cell_type":"markdown","source":["과대적합은 학습할 샘플이 너무 적어 새로운 데이터에 일반화 시킬수 있는 모델을 훈련시킬 수 없기 때문에 발생한다.\n","\n","\n","무한히 많은 데이터가 주어지면 데이터 분포의 모든 가능한 측면을 모델이 학습할 수 있을 것이다. \n","\n","\n","데이터 증식은 기존의 훈련 샘플로부터 더 많은 훈련 데이터를 생성하는 방법이다. 이 방법은 그럴듯한 이미지를 생성하도록 여러 가지 랜덤한 변환을 적용하여 샘플을 늘린다.\n","\n","\n","훈련 시에 모델이 정확히 같은 데이터를 두 번 만나지 않도록 하는 것이 목표이다. 모델이 데이터의 여러 측면을 학습하면 일반화에 도움이 될 것이다."],"metadata":{"id":"FbMwgVV7rKYw"}},{"cell_type":"markdown","source":["케라스에서는 ImageDataGenerator가 읽은 이미지에 여러 종류의 랜덤 변환을 적용하도록 설정할 수 있다."],"metadata":{"id":"afYOAfSurux3"}},{"cell_type":"code","source":["datagen = ImageDataGenerator(\n","      rotation_range=40,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')"],"metadata":{"id":"0NFkvC2vqL3N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["추가적인 매개변수\n","- rotation_range는 랜덤하게 사진을 회전시킬 각도 범위(0~180 사이의 값)\n","\n","\n","- width_shift_range와 height_shift_range는 사진을 수평과 수직으로 랜덤하게 평행 이동시킬 범위(전체 넓이와 높이에 대한 비율)\n","\n","\n","- shear_range는 랜덤하게 전단 변환을 적용할 각도 범위\n","\n","\n","- zoom_range는 랜덤하게 사진을 확대할 범위\n","\n","\n","- horizontal_flip은 랜덤하게 이미지를 수평으로 뒤집는다 수평 대칭을 가정할 수 있을 때 사용(예를 들어, 풍경/인물 사진)\n","\n","\n","- fill_mode는 회전이나 가로/세로 이동으로 인해 새롭게 생성해야 할 픽셀을 채울 전략이다."],"metadata":{"id":"PzOAygicr2Qa"}},{"cell_type":"code","source":["# 이미지 전처리 유틸리티 모듈\n","from keras.preprocessing import image\n","import matplotlib.pyplot as plt\n","\n","fnames = sorted([os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)])\n","\n","# 증식할 이미지 선택합\n","img_path = fnames[3]\n","\n","# 이미지를 읽고 크기를 변경\n","img = image.load_img(img_path, target_size=(150, 150))\n","\n","# (150, 150, 3) 크기의 넘파이 배열로 변환\n","x = image.img_to_array(img)\n","\n","# (1, 150, 150, 3) 크기로 변환\n","x = x.reshape((1,) + x.shape)\n","\n","# flow() 메서드는 랜덤하게 변환된 이미지의 배치를 생성\n","# 무한 반복되기 때문에 어느 지점에서 중지해야함\n","i = 0\n","for batch in datagen.flow(x, batch_size=1):\n","    plt.figure(i)\n","    imgplot = plt.imshow(image.array_to_img(batch[0]))\n","    i += 1\n","    if i % 4 == 0:\n","        break\n","\n","plt.show()"],"metadata":{"id":"O-nWLQ5KqL0m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dropout 층 추가\n","\n"],"metadata":{"id":"xlUCKHrEtrBP"}},{"cell_type":"markdown","source":["- 데이터 증식을 사용하여 새로운 네트워크를 훈련시킬 때 네트워크에 같은 입력 데이터가 두 번 주입되지 않게 한다.\n","\n","- 하지만 적은 수의 원본 이미지에서 만들어졌기 때문에 여전히 입력 데이터들 사이에 상호 연관성이 크다.\n","\n","- 즉, 새로운 정보를 만들어낼 수 없고 단지 기존 정보의 재조합만 가능하다.\n","\n","- 그렇기 때문에 완전히 과대적합을 제거하기에 충분하지 않을 수 있다.\n","\n","- 과대적합을 더 억제하기 위해 완전 연결 분류기 직전에 Dropout 층을 추가한다."],"metadata":{"id":"Zvv_1wPctv4o"}},{"cell_type":"code","source":["from tensorflow.keras import optimizers\n","from tensorflow.keras import layers, models\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu',\n","                        input_shape=(150, 150, 3)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Flatten())\n","model.add(layers.Dropout(0.5))\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])"],"metadata":{"id":"wB4milsWqLyD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 데이터 증식"],"metadata":{"id":"pv3MUSihuJMf"}},{"cell_type":"code","source":["train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,)\n","\n","# 검증 데이터는 증식하면 안됌\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","        # 타깃 디렉터리\n","        train_dir,\n","        # 모든 이미지를 150 × 150 크기로 변경\n","        target_size=(150, 150),\n","        batch_size=32,\n","        # binary_crossentropy 손실을 사용하기 때문에 이진 레이블을 생성\n","        class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(150, 150),\n","        batch_size=32,\n","        class_mode='binary')\n","\n","history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=100,\n","      epochs=100,\n","      validation_data=validation_generator,\n","      validation_steps=50)"],"metadata":{"id":"lq7ZxgGjvNOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.save('cats_and_dogs_small_2.h5')"],"metadata":{"id":"pBZO7H92qLvw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 시각화"],"metadata":{"id":"kqoRX1q6vYcA"}},{"cell_type":"code","source":["acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"metadata":{"id":"w1oeBovyqLuL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["데이터 증식과 드롭아웃 덕분에 더이상 과대적합되지 않았다.\n","\n","훈련 곡선이 검증 곡선에 가깝게 따라가고 있다. \n","\n","검증 데이터에서 82% 정확도를 달성하였다.\n","\n","규제하지 않은 모델과 비교했을 때 15% 정도 향상되었다.\n","\n","다른 규제 기법을 더 사용하고 네트워크의 파라미터를 튜닝하면(합성곱 층의 필터 수나 네트워크의 층의 수 등) 86%나 87% 정도까지 더 높은 정확도를 얻을 수도 있다.\n","\n","하지만 데이터가 적기 때문에 컨브넷을 처음부터 훈련해서 더 높은 정확도를 달성하기는 어렵다. \n","\n","이런 상황에서 정확도를 높이기 위한 다음 단계는 사전 훈련된 모델을 사용하는 것이 차선책이다."],"metadata":{"id":"ffqgSFlWvcTZ"}}]}