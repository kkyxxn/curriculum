{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b1bffd",
   "metadata": {},
   "source": [
    "### 참고 URL\n",
    "- https://velog.io/@jhlee508/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-KNNK-Nearest-Neighbor-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49008298",
   "metadata": {},
   "source": [
    "### K-NN 알고리즘\n",
    "- K-NN 알고리즘은 머신러닝 알고리즘 중 지도학습에 속한다. 지도학습이란 쉽게 말하여 레이블(정답)을 주고 학습을 시키는 것을 의미한다.\n",
    "- 지도학습은 크게 회귀(Regression)와 분류(Classification)로 나뉘는데 K-NN 알고리즘은 지도학습 중 분류에 속한다.\n",
    "- 데이터를 가장 가까운 속성에 따라 분류하여 레이블링을 하는 알고리즘이다.\n",
    "- 데이터 레이블링 할 때에 K의 값에 따라 결과가 달라지며 잘 지정해주어야 한다는 것을 명심해야한다.\n",
    "- 보편적으로 K의 값을 정할 때 동률이 나오지 않도록 홀수로 지정한다.\n",
    "- K-NN은 K-Nearest Neighbor의 줄임말인데, 말 그대로 K개의 가까운 이웃의 속성에 따라 분류한다.\n",
    "- K-NN 알고리즘은 어떤 새로운 데이터로부터 거리가 가까운 K개의 다른 데이터의 레이블(속성)을 참고하여 K개의 데이터 중 가장 빈도 수가 높게 나온 데이터의 레이블로 분류하는 알고리즘이다.\n",
    "-  K-NN 알고리즘과 같은 거리기반 모델의 경우에는 구현 시 변수 값의 범위를 재조정 해주어야 한다. 거리를 잴 때에는 숫자를 다루므로 변수 값 범위 재조정를 해주어야 변수의 중요도를 고르게 해석할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbdc4a",
   "metadata": {},
   "source": [
    "### 거리 측정 방법\n",
    "1. 유클리드 거리(Euclidean Distance)\n",
    "     - K-NN 알고리즘은 거리를 측정할 때 일반적으로 '유클리드 거리(Euclidean Distance)' 계산법을 사용한다. 2차원 평면에 서로 다른 두 점 A(x1, y1)와 B(x2, y2)가 있을 때 이 둘의 거리 d는 유클리드 거리 계산법에 의해 다음과 같이 나온다. \n",
    "         * x1 -x2의 제곱 + y1 - y2의 제곱 합에 루트</br></br>\n",
    "         \n",
    "2. 맨해튼 거리(Manhattan Distance)\n",
    "    - K-NN 알고리즘 내부의 거리 측정 방법으로 흔히 사용된다.\n",
    "        * d(A,B)=∣x1−x2∣+∣y1−y2∣"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c0370",
   "metadata": {},
   "source": [
    "### 변수 값 범위 재조정 방법\n",
    "1. 최소-최대 정규화(min-max normalization)\n",
    "    - 최소-최대 정규화는 변수의 범위를 0%에서 100%까지로 나타낸다. 공식은 다음과 같다.\n",
    "             Z=(X−min(X))/(max(X)−min(X))\n",
    " \n",
    "2. z-점수 표준화(z-score standardization)\n",
    "    - z-점수 표준화는 변수의 범위를 정규분포화하여 평균을 0, 포준편차가 1이 되도록 한다.\n",
    "             Z=(X−평균)/표준편차\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7b24c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "t_df = pd.read_pickle('./dataset/t_df.pkl')\n",
    "\n",
    "X = t_df.drop('survived', axis=1) # 독립변수, 설명변수\n",
    "y = t_df['survived'] # 종속변수, 결과변수\n",
    "\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1b7637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 예측 정확도 :  0.767175572519084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k_model = KNeighborsClassifier(n_neighbors =5) # k 설정\n",
    "k_model.fit(X_train, y_train)\n",
    "k_pred = k_model.predict(X_test)\n",
    "\n",
    "k_accuracy = accuracy_score(y_test, k_pred) # 실제값, 예측값이 얼마나 같은지를 비교 -> accuracy\n",
    "print('KNN 예측 정확도 : ', k_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
