{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ebf5c7",
   "metadata": {},
   "source": [
    "### 텍스트 분석\n",
    "-  NLP(National Language Processing): 인간의 언어를 이해하고 해석하는데 중점(기계 번역, 챗봇 등)\n",
    "- TA(Text Analytics): 비정형 텍스트에서 의미 있는 정보를 추출하는데 중점(결국, NLP를 포함하는 상위 개념) -> 머신러닝, 통계 등을 활용해 모델을 만들고 비즈니스 인텔리전스 등 예측 분석을 수행\n",
    "    - 텍스트 분류(Text Classification): Text Categorization, 문서가 특정 분류 또는 카테고리에 있는 것을 예측하는 기법\n",
    "    - 감성 분석(Sentiment Analysis): Text에 나타나는 감정/판단 등의 주관적인 요소를 분석하는 기법\n",
    "    - 텍스트 요약(Text Summarization): Text에 중요한 주제나 중심 사상을 추출하는 기법(Topic Modeling)\n",
    "    - 텍스트 군집화(Text Clusterting)와 유사도 측정: 비슷한 유형의 문서에 대해 군집화를 수행하는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92665a9",
   "metadata": {},
   "source": [
    "- 머신러닝 알고리즘은 숫자형 피처 기반 데이터를 입력받을 수 있음\n",
    "- 어떻게 비정형 텍스트 데이터를 숫자형 피처 기반으로 추출하여 의미 있는 값을 부여하는가?</br></br>\n",
    "### 피처 벡터화(Feature Vectorization), 피처 추출(Feature Extraction)\n",
    "- 텍스트 분석 수행 프로세스\n",
    "    1. 텍스트 전처리: 텍스트를 피처로 만들기 전 대/소문자 변경, 특수문자 삭제, 단어(word) 토큰화, 의미 없는 단어 삭제, 어근 추출 등 텍스트 정규화 작업\n",
    "    2. 피처 벡터화/추출: 가공된 텍스트에서 피처를 추출하고 벡터 값을 할당하는 작업, BOW와 Word2Vec이 있음\n",
    "    3. ML 모델 수립/학습/평가: 피처 벡터화된 데이터 세트에 ML 모델을 적용</br></br>\n",
    "    \n",
    "- 파이썬 기반의 NLP, 텍스트 분석 패키지\n",
    "    - NLTK(National Language Toolkit for Python): 파이썬의 가장 대표적인 NLP 패키지, 수행 속도가 느려 실제 대량의 데이터 기반에서는 제대로 활용되지 못함\n",
    "    - Gensim: 토픽 모델링 분야의 대표적인 패키지, SpaCy와 가장 많이 사용되는 NLP 패키지\n",
    "    - SpaCy: 수행성능이 가장 좋은 최신의 NLP 패키지, 가장 많이 쓰임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce90fb34",
   "metadata": {},
   "source": [
    "### 텍스트 전처리(텍스트 정규화)\n",
    "- 텍스트 정규화 작업\n",
    "    - 클렌징(Cleansing)\n",
    "       - 텍스트 분석에 방해되는 불필요한 문자, 기호 등을 사전에 제거하는 작업(HTML, XML 태그, 특정기호 등)\n",
    "    - 토큰화(Tokenization)\n",
    "      - 문서에서 문장을 분리하는 문장 토큰화, 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
    "      - 문장 토큰화(Sentence Tokenization)\n",
    "      - 단어 토큰화(Word Tokenization): 공백, 콤마(,), 마침표(.), 개행 문자 등으로 단어를 분리\n",
    "      - 문장을 단어 토큰화 할 경우 문맥적인 의미는 무시되기 마련인데 이를 보정해주는 것 = n-gram\n",
    "      - n-gram: 연속된 n개의 단어를 하나의 토큰화 단위로 분리하여 문맥적인 의미를 포함시킴\n",
    "    - 필터링/스톱 워드 제거/철자 수정\n",
    "        - 스톱 워드(Stop Word) 제거 \n",
    "        - 분석에 큰 의미가 없는 단어 제거 > NLTK의 stopwords 목록을 사용, nltk.download('stopwords')\n",
    "    - Stemming/Lemmatization\n",
    "        - 문법적 혹은 의미적으로 변화하는 단어의 원형을 찾는 것(과거형, 현재형, 미래형, 3인칭 단수 등)\n",
    "        - 성능: Lemmatization > Stemming, 수행 시간: Lemmatization > Stemming\n",
    "        - NLTK Stemmer: Porter, Lancaster, Snowball Stemmer / NLTK Lemmatization: WordNetLemmatizer\n",
    "        - Stemmer(Lancaster)\n",
    "        - Lemmatization(WordNetLemmatizer), 품사를 입력해줘야 함(동사:'v', 형용사:'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742eef8",
   "metadata": {},
   "source": [
    "### BOW(Bag of Words)\n",
    "- 모든 단어(Words)를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델\n",
    "- 모든 단어를 봉투(Bag) 안에 넣은 뒤 흔들어서 섞는다는 의미로 BOW 모델이라고 함\n",
    "- BOW 단점\n",
    "\n",
    "    - 문맥 의미(Sementic Context) 반영 부족: 단어의 순서를 고려하지 않으므로 문맥적 의미가 무시됨                                                                            (보완을 위한 n-gram 기법이 있긴 있음)\n",
    "    - 희소 행렬 문제(희소성, 희소행렬): 희소 행렬 형태의 데이터 세트가 만들어지기 쉬움. 즉 매우 많은 문서는 서로 다른 단어로 구성되기 때문에 단어의 빈도가 나타나지 않는 경우가 훨씬 더 많은데, 이로 인해 대부분의 데이터는 0으로 채워지게 됨. 대규모의 행렬에서 대부분의 값이 0인 행렬을 희소 행렬(Sparse Matrix)이라고 하며 이는 ML 알고리즘의 수행 시간과 예측 성능을 떨어뜨린다.(희소 행렬 반대 → 밀집 행렬(Dense Matrix))</br></br>\n",
    "    \n",
    "- BOW 피처 벡터화\n",
    "\n",
    "    - 모든 문서의 모든 단어를 칼럼 형태를 나열하고, 각 문서에서 해당 단어의 횟수 혹은 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것\n",
    "    - M개의 텍스트 문서, 문서 별 N개의 단어 → M×N개의 단어 피처로 이루어진 행렬을 구성\n",
    "    - 대표적인 BOW 피처 벡터화: Count 벡터화, TF-IDF(Term Frequency - Inverse Document Frequency) 벡터화\n",
    "    - Count 벡터화: 개별 문서에서 해당 단어가 나타나는 횟수가 많을수록 중요한 단어로 인식\n",
    "    - TF-IDF 벡터화: 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 전체 문서에서 자주 나타나는 단어에                          대해서는 페널티를 부여\n",
    "\n",
    "문서의 개수가 많은 경우, TF-IDF 방식이 성능이 좋음\n",
    "\n",
    "- 사이킷런의 Count 및 TF-IDF 벡터화 구현(CountVectorizer, TfidfVectorizer)\n",
    "  - CountVectorizer 클래스(텍스트 전처리 및 피처 벡터화 함께 수행, fit_transform())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3aff6",
   "metadata": {},
   "source": [
    "<img src='./dataset/parameter.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104a0dc",
   "metadata": {},
   "source": [
    "<img src ='./dataset/process.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd502a0",
   "metadata": {},
   "source": [
    "### BOW 벡터화를 위한 희소 행렬\n",
    "\n",
    "- 피처 칼럼을 만들 때 대규모 행렬이 생성되고, 이 대규모 행렬의 대부분을 0이 차지하는 행렬 → 희소 행렬\n",
    "- 희소 행렬은 불필요한 0 때문에 연산에 많은 시간이 소요됨\n",
    "- 희소 행렬을 적은 메모리 공간을 차지할 수 있도록 변환해야 함(COO형식, CSR 형식)\n",
    "\n",
    "##### 희소 행렬(COO형식- Coordinate)\n",
    "- 0이 아닌 데이터만 별도의 데이터 배열(Array)에 저장하고, 그 데이터가 가리키는 행렬의 위치를 별도 배열에 저장하는 방식\n",
    "- [[3,0,2],[0,1,0]]에서 0이 아닌 데이터는 [3,2,1], 0이 아닌 데이터가 있는 위치는 (row,col) (0, 0), (0, 2), (1,1)\n",
    "- 행 위치 배열 [0,0,1], 열 위치 배열 [0,2,1]\n",
    "- 파이썬에서는 희소 행렬 변환을 위해 Scipy의 sparse 패키지 활용\n",
    "\n",
    "##### 희소 행렬(CSR형식 - Compressed Sparse Row)\n",
    "\n",
    "- COO 형식의 반복적인 위치 데이터 사용 문제를 해결\n",
    "- 행 위치 배열은 특성상 0부터 순차적으로 증가하는 값으로 이루어짐\n",
    "- 행 위치 배열 [0,0,1,1,1,1,1,2,2,3,4,4,5]은 0이 2번, 1이 5번, 2가 2번, 4가 2번 반복되고 있음\n",
    "- 행 위치 배열의 숫자가 변하는 위치만 다시 별도의 배열로 변환하는 방식\n",
    "- [0,0,1,1,1,1,1,2,2,3,4,4,5] → [0,2,7,9,10,12] CSR변환 후 최종 데이터 총 개수 배열 추가 [0,2,7,9,10,12,13]\n",
    "- Scipy의 csr_matrix 패키지 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d9e3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
      "Collecting click\n",
      "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp38-cp38-win_amd64.whl (274 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\home\\anaconda3\\envs\\cakd5\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\home\\anaconda3\\envs\\cakd5\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.0.4 nltk-3.7 regex-2022.3.15 tqdm-4.63.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5e2b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HOME\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "# 문장 토큰화는 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 경우에 사용\n",
    "sentences = sent_tokenize(text = text_sample) # 각각의 문장으로 구성된 list 객체 반환\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374b29df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# 문장을 단어로 토큰화 -> 공백, 콤마, 마침표, 개행문자, 정규표현식 등으로 분리 \n",
    "# 단어의 순서가 중요하지 않는 경우 단어 토큰화만 사용해도 충분\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3314e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "\n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "# 여러 문장에 대해 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)\n",
    "\n",
    "# 3개의 문장을 문장별로 먼저 토큰화 했으므로 word_tokens 변수는 3개의 리스트 객체를 내포하는 리스트\n",
    "# 내포된 리스트 객체는 각각 문장별로 토큰화된 단어를 요소로 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151962a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HOME\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스톱 워드 제거 -> 문맥적으로 큰 의미가 없는 단어\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3b3c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# 문맥적 큰 의미가 없는 단어가 빈번하게 나타나면 제거해줘야함 why? 빈번하게 나타나면 중요한 단어로 인지 될 수 있으므로\n",
    "print('영어 stop words 개수 : ', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383ec5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [과제] 위의 word_tokens에서 stopwords를 제거한 후 출력하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20b4a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "# 상기 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "\n",
    "# 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
    "for word in sentence:\n",
    "    # 소문자로 모두 변환\n",
    "    word = word.lower()\n",
    "    # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "    if word not in stopwords:\n",
    "        filtered_words.append(word)\n",
    "all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecd037b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "# Steming and Lematization\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer() # 객체 생성\n",
    "\n",
    "# steming 정확한 원형을 찾지 못하고 원형 단어에서 철자가 다른 어근 단어로 인식하는 경우가 발생\n",
    "# 의미론적인것들을 반영못하고 형태론적인것만 반영\n",
    "print(stemmer.stem('working'), stemmer.stem('work'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34b49698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HOME\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# stemmer 보다 정확하게 원형 단어 추출\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31bb1919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1 \n",
      "\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 희소 행렬 - COO 형식\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                 [1,4,0,3,2,5],\n",
    "                 [0,6,0,3,0,0],\n",
    "                 [2,0,0,0,0,0],\n",
    "                 [0,0,0,7,0,8],\n",
    "                 [1,0,0,0,0,0]])\n",
    "# 0이 아닌 데이터 추출\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성.\n",
    "row_pos = [0,0,1,1,1,1,1,2,2,3,4,4,5]\n",
    "col_pos = [2,5,0,1,3,4,5,1,3,0,3,5,0]\n",
    "\n",
    "# COO형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "print(sparse_coo,'\\n')\n",
    "print(sparse_coo.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "448c3814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1 \n",
      "\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])  # 끝 인덱스에서 +1 하나 더 넣어줌\n",
    "\n",
    "sparse_csr = sparse.csr_matrix((data2,col_pos,row_pos_ind))\n",
    "print(sparse_csr,'\\n')\n",
    "print(sparse_csr.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
